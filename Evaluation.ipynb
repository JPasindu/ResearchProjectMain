{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e42c81-7203-4e38-9d1c-6daf73d738b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Data_Generation_2 import data_sets\n",
    "import ZKIPModel_EM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from math import lgamma\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, r2_score\n",
    "import ZkICMP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import poisson\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4119fd-aa99-4b0d-9bf9-109159896d33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ZKIPModel_EM(model, X_train, X_test, y_train, y_test, true_params=None, plot_name=None, pi1=0.4, pi2=0.4):\n",
    "    if plot_name is  None:\n",
    "        plot_name=f'{int(X_train.shape[0]/80*100)}, {X_train.shape[1]}_({pi1:.2f},{pi2:.2f})'\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"ZKIPModel_EM_({plot_name})\")\n",
    "    print(\"-\" * 30)\n",
    "    \"\"\"Evaluate ZKIP model performance\"\"\"\n",
    "    # Predict expected values\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "    y_pred_expected = model.predict_expected(X_test)\n",
    "    y_pred_mode = model.predict_mode(X_test)\n",
    "    \n",
    "    # Calculate metrics for expected value predictions\n",
    "    mse_expected = mean_squared_error(y_test, y_pred_expected)\n",
    "    mae_expected = mean_absolute_error(y_test, y_pred_expected)\n",
    "    r2 = r2_score(y_test, y_pred_expected)\n",
    "    mse_mode = mean_squared_error(y_test, y_pred_mode)\n",
    "    mae_mode = mean_absolute_error(y_test, y_pred_mode)\n",
    "    r2_mode = r2_score(y_test, y_pred_mode)\n",
    "    \n",
    "    # For mode predictions, we need to check accuracy\n",
    "    accuracy_mode = np.mean(y_test == y_pred_mode)\n",
    "    print(\"Performance metrics:\")\n",
    "    print(f\"  Expected value prediction:\")\n",
    "    print(f\"    MSE: {mse_expected:.4f}\")\n",
    "    print(f\"    MAE: {mae_expected:.4f}\")\n",
    "    print(f\"    r^2: {r2:.4f}\")\n",
    "    print(f\"  Mode prediction:\")\n",
    "    print(f\"    MSE: {mse_mode:.4f}\")\n",
    "    print(f\"    MAE: {mse_mode:.4f}\")\n",
    "    print(f\"    r^2: {r2_mode:.4f}\")\n",
    "    print(f\"    Accuracy: {accuracy_mode:.4f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    # Plot 1: True vs predicted (expected)\n",
    "    axes[0, 0].scatter(y_test, y_pred_expected, alpha=0.6)\n",
    "    axes[0, 0].plot([0, max(y_test)], [0, max(y_test)], 'r--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('True Count')\n",
    "    axes[0, 0].set_ylabel('Predicted Expected Count')\n",
    "    axes[0, 0].set_title('True vs Predicted (Expected Value)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    # Plot 2: True vs predicted (mode)\n",
    "    axes[0, 1].scatter(y_test, y_pred_mode, alpha=0.6)\n",
    "    axes[0, 1].plot([0, max(y_test)], [0, max(y_test)], 'r--', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('True Count')\n",
    "    axes[0, 1].set_ylabel('Predicted Mode')\n",
    "    axes[0, 1].set_title('True vs Predicted (Mode)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_test - y_pred_expected\n",
    "    axes[1, 0].scatter(y_pred_expected, residuals, alpha=0.6)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Predicted Expected Count')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    # Plot 4: Count distribution comparison\n",
    "    axes[1, 1].hist(y_test, bins=range(0, int(max(y_test)) + 2), alpha=0.7, label='True', density=True)\n",
    "    axes[1, 1].hist(y_pred_mode, bins=range(0, int(max(y_test)) + 2), alpha=0.7, label='Predicted Mode', density=True)\n",
    "    axes[1, 1].set_xlabel('Count')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Count Distribution: True vs Predicted')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/ZKIPModel_EM_({plot_name}).png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956138db-d11e-472f-8c64-c3bd48bb2c4c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ZkICMP(model, X_train, X_test, y_train, y_test, plot_name=None, pi1=0.4, pi2=0.4):\n",
    "    if plot_name is  None:\n",
    "        plot_name=f'{int(X_train.shape[0]/80*100)}, {X_train.shape[1]}_({pi1:.2f},{pi2:.2f})'\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"ZkICMP_({plot_name})\")\n",
    "    print(\"-\" * 30)\n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "    y_pred_counts, y_pred_probs, y_pred_rates, count_values = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_rates = mean_squared_error(y_test, y_pred_rates)\n",
    "    mae_rates = mean_absolute_error(y_test, y_pred_rates)\n",
    "    r2_rates = r2_score(y_test, y_pred_rates)\n",
    "    mse_counts = mean_squared_error(y_test, y_pred_counts)\n",
    "    mae_counts = mean_absolute_error(y_test, y_pred_counts)\n",
    "    r2_counts = r2_score(y_test, y_pred_counts)\n",
    "    accuracy = accuracy_score(y_test, y_pred_counts)\n",
    "    \n",
    "    print(\"Performance metrics:\")\n",
    "    print(f\"  Rate predictions (λ):\")\n",
    "    print(f\"    MSE: {mse_rates:.4f}\")\n",
    "    print(f\"    MAE: {mae_rates:.4f}\")\n",
    "    print(f\"    r^2: {r2_rates:.4f}\")\n",
    "\n",
    "    print(f\"  Count predictions:\")\n",
    "    print(f\"    MSE: {mse_counts:.4f}\")\n",
    "    print(f\"    MAE: {mae_counts:.4f}\")\n",
    "    print(f\"    r^2: {r2_counts:.4f}\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: True vs predicted rates\n",
    "    axes[0, 0].scatter(y_test, y_pred_rates, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_rates))\n",
    "    axes[0, 0].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('True Count')\n",
    "    axes[0, 0].set_ylabel('Predicted Rate (λ)')\n",
    "    axes[0, 0].set_title('True Count vs Predicted CMP Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: True vs predicted counts\n",
    "    axes[0, 1].scatter(y_test, y_pred_counts, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_counts))\n",
    "    axes[0, 1].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('True Count')\n",
    "    axes[0, 1].set_ylabel('Predicted Count')\n",
    "    axes[0, 1].set_title('True vs Predicted Counts')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_test - y_pred_rates\n",
    "    axes[1, 0].scatter(y_pred_rates, residuals, alpha=0.6)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Predicted Rate (λ)')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Count distribution comparison\n",
    "    max_bin = int(max(np.max(y_test), np.max(y_pred_counts))) + 2\n",
    "    bins = range(0, min(max_bin, 50))\n",
    "    axes[1, 1].hist(y_test, bins=bins, alpha=0.7, label='True', density=True)\n",
    "    axes[1, 1].hist(y_pred_counts, bins=bins, alpha=0.7, label='Predicted', density=True)\n",
    "    axes[1, 1].set_xlabel('Count')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Count Distribution: True vs Predicted')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/ZKICMP_({plot_name}).png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d01078-56aa-49fe-bfe7-44918ee028af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_HistGradientBoostingRegressor(model, X_train, y_train, X_test, y_test, plot_name=None, pi1=0.4, pi2=0.4):\n",
    "    if plot_name is  None:\n",
    "        plot_name=f'{int(X_train.shape[0]/80*100)}, {X_train.shape[1]}_({pi1:.2f},{pi2:.2f})'\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"HistGradientBoostingRegressor_({plot_name})\")\n",
    "    print(\"-\" * 30)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # Predict expected values (Poisson rates λ)\n",
    "    y_pred_rates = model.predict(X_test)\n",
    "\n",
    "    # Convert rates to integer counts (by rounding)\n",
    "    y_pred_counts = np.round(y_pred_rates).astype(int)\n",
    "\n",
    "    # Calculate metrics for rate predictions\n",
    "    mse_rates = mean_squared_error(y_test, y_pred_rates)\n",
    "    mae_rates = mean_absolute_error(y_test, y_pred_rates)\n",
    "    r2_rates = r2_score(y_test, y_pred_rates)\n",
    "\n",
    "    # Metrics for count predictions\n",
    "    mse_counts = mean_squared_error(y_test, y_pred_counts)\n",
    "    mae_counts = mean_absolute_error(y_test, y_pred_counts)\n",
    "    r2_counts = r2_score(y_test, y_pred_counts)\n",
    "\n",
    "    # Poisson-specific metrics\n",
    "    def poisson_deviance(y_true, y_pred):\n",
    "        \"\"\"Poisson deviance - better for count data than MSE\"\"\"\n",
    "        # Avoid log(0) and division by zero issues\n",
    "        mask = (y_true > 0) & (y_pred > 0)\n",
    "        if np.any(mask):\n",
    "            deviance = 2 * np.mean(y_true[mask] * np.log(y_true[mask] / y_pred[mask]) - (y_true[mask] - y_pred[mask]))\n",
    "        else:\n",
    "            # Fallback to MSE if no valid pairs\n",
    "            deviance = np.mean((y_true - y_pred) ** 2)\n",
    "        return deviance\n",
    "\n",
    "    deviance = poisson_deviance(y_test, y_pred_rates)\n",
    "\n",
    "    print(\"Performance metrics:\")\n",
    "    print(f\"  Rate predictions (λ):\")\n",
    "    print(f\"    MSE: {mse_rates:.4f}\")\n",
    "    print(f\"    MAE: {mae_rates:.4f}\")\n",
    "    print(f\"    R²: {r2_rates:.4f}\")\n",
    "\n",
    "    print(f\"  Count predictions (rounded):\")\n",
    "    print(f\"    MSE: {mse_counts:.4f}\")\n",
    "    print(f\"    MAE: {mae_counts:.4f}\")\n",
    "    print(f\"    R²: {r2_counts:.4f}\")\n",
    "    accuracy = (y_pred_counts==y_test).sum()/(y_pred_counts!=y_test).sum()\n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: True vs predicted rates\n",
    "    axes[0, 0].scatter(y_test, y_pred_rates, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_rates))\n",
    "    axes[0, 0].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('True Count')\n",
    "    axes[0, 0].set_ylabel('Predicted Rate (λ)')\n",
    "    axes[0, 0].set_title('True Count vs Predicted Poisson Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: True vs predicted counts\n",
    "    axes[0, 1].scatter(y_test, y_pred_counts, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_counts))\n",
    "    axes[0, 1].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('True Count')\n",
    "    axes[0, 1].set_ylabel('Predicted Count (Rounded)')\n",
    "    axes[0, 1].set_title('True vs Predicted Counts')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_test - y_pred_rates\n",
    "    axes[1, 0].scatter(y_pred_rates, residuals, alpha=0.6)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Predicted Rate (λ)')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Count distribution comparison\n",
    "    max_bin = int(max(np.max(y_test), np.max(y_pred_counts))) + 2\n",
    "    bins = range(0, min(max_bin, 50))  # Limit bins to avoid too many\n",
    "    axes[1, 1].hist(y_test, bins=bins, alpha=0.7, label='True', density=True)\n",
    "    axes[1, 1].hist(y_pred_counts, bins=bins, alpha=0.7, label='Predicted', density=True)\n",
    "    axes[1, 1].set_xlabel('Count')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Count Distribution: True vs Predicted')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/HistGradientBoostingRegressor_({plot_name}).png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec870bf-5d59-4f6b-a1d7-860016a47aa4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_rf(rf, X_train, X_test, y_train, y_test, plot_name=None, pi1=0.4, pi2=0.4):\n",
    "    if plot_name is  None:\n",
    "        plot_name=f'({int(X_train.shape[0]/80*100)}, {X_train.shape[1]})_({pi1:.2f},{pi2:.2f})'\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"PoissonRandomForestRegressor_{plot_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    rf.fit(X_train, y_train)\n",
    "    # Get rate predictions (λ)\n",
    "    y_pred_rates = rf.predict(X_test)\n",
    "\n",
    "    # Get count predictions (rounded)\n",
    "    y_pred_counts = np.round(y_pred_rates).astype(int)\n",
    "\n",
    "    # Calculate metrics for rates\n",
    "    mse_rates = mean_squared_error(y_test, y_pred_rates)\n",
    "    mae_rates = mean_absolute_error(y_test, y_pred_rates)\n",
    "    r2_rates = r2_score(y_test, y_pred_rates)\n",
    "    deviance = mean_poisson_deviance(y_test, y_pred_rates)\n",
    "\n",
    "    # Calculate metrics for counts\n",
    "    mse_counts = mean_squared_error(y_test, y_pred_counts)\n",
    "    mae_counts = mean_absolute_error(y_test, y_pred_counts)\n",
    "    r2_counts = r2_score(y_test, y_pred_counts)\n",
    "    \n",
    "    print(\"Performance metrics:\")\n",
    "    print(f\"  Rate predictions (λ):\")\n",
    "    print(f\"    MSE: {mse_rates:.4f}\")\n",
    "    print(f\"    MAE: {mae_rates:.4f}\")\n",
    "    print(f\"    R²: {r2_rates:.4f}\")\n",
    "    \n",
    "    print(f\"  Count predictions (rounded):\")\n",
    "    print(f\"    MSE: {mse_counts:.4f}\")\n",
    "    print(f\"    MAE: {mae_counts:.4f}\")\n",
    "    print(f\"    R²: {r2_counts:.4f}\")\n",
    "    accuracy = (y_pred_counts == y_test).sum() / len(y_test) \n",
    "    print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: True vs predicted rates\n",
    "    axes[0, 0].scatter(y_test, y_pred_rates, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_rates))\n",
    "    axes[0, 0].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('True Count')\n",
    "    axes[0, 0].set_ylabel('Predicted Rate (λ)')\n",
    "    axes[0, 0].set_title('True Count vs Predicted Poisson Rate')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: True vs predicted counts\n",
    "    axes[0, 1].scatter(y_test, y_pred_counts, alpha=0.6)\n",
    "    max_val = max(np.max(y_test), np.max(y_pred_counts))\n",
    "    axes[0, 1].plot([0, max_val], [0, max_val], 'r--', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('True Count')\n",
    "    axes[0, 1].set_ylabel('Predicted Count (Rounded)')\n",
    "    axes[0, 1].set_title('True vs Predicted Counts')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_test - y_pred_rates\n",
    "    axes[1, 0].scatter(y_pred_rates, residuals, alpha=0.6)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Predicted Rate (λ)')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Count distribution comparison\n",
    "    max_bin = int(max(np.max(y_test), np.max(y_pred_counts))) + 2\n",
    "    bins = range(0, min(max_bin, 50))  # Limit bins to avoid too many\n",
    "    axes[1, 1].hist(y_test, bins=bins, alpha=0.7, label='True', density=True)\n",
    "    axes[1, 1].hist(y_pred_counts, bins=bins, alpha=0.7, label='Predicted', density=True)\n",
    "    axes[1, 1].set_xlabel('Count')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Count Distribution: True vs Predicted')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    if plot_name is  None:\n",
    "        plot_name=f'{int(X_train.shape[0]/80*100)}, {X_train.shape[1]}'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/PoissonRandomForestRegressor_({plot_name}).png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb37f71d-f542-4426-8d95-038a418e3290",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_all(n, pi1, pi2):\n",
    "    data_sets(n=n, k_inflated=3, true_beta=None, true_gamma=None, true_delta=None, \n",
    "                      pi1=pi1, pi2=pi1, pi3=None, seed=42)\n",
    "    \n",
    "    X_train_name=f'DataSets/X_train_0k_inflated_({n}, 2)_({pi1:.2f},{pi2:.2f}).csv'\n",
    "    X_test_name=f'DataSets/X_test_0k_inflated_({n}, 2)_({pi1:.2f},{pi2:.2f}).csv'\n",
    "    y_train_df_name=f'DataSets/y_train ({n}, 2)_({pi1:.2f},{pi2:.2f}).csv'\n",
    "    y_test_df_name=f'DataSets/y_test ({n}, 2)_({pi1:.2f},{pi2:.2f}).csv'\n",
    "    \n",
    "    X_train = pd.read_csv(X_train_name)\n",
    "    X_test = pd.read_csv(X_test_name)\n",
    "    y_train = pd.read_csv(y_train_df_name)\n",
    "    y_test = pd.read_csv(y_test_df_name)\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    # Convert to numpy arrays for ZkICMP\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    \n",
    "    # ZKIPModel_EM\n",
    "    model = ZKIPModel_EM.ZKIPModel_EM(k_inflated=3) \n",
    "    evaluate_ZKIPModel_EM(model, X_train, X_test, y_train, y_test, pi1=pi1, pi2=pi2)\n",
    "    \n",
    "    # ZkICMP\n",
    "    model = ZkICMP.ZkICMP(k=3)\n",
    "    evaluate_ZkICMP(model, X_train, X_test, y_train, y_test, pi1=pi1, pi2=pi2)\n",
    "    \n",
    "    # HistGradientBoostingRegressor\n",
    "    poisson_GB = HistGradientBoostingRegressor(\n",
    "            loss='poisson',        # This is the key - uses Poisson likelihood\n",
    "            random_state=42,\n",
    "            max_iter=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=20    # Good for count data to prevent overfitting\n",
    "        )\n",
    "    evaluate_HistGradientBoostingRegressor(poisson_GB, X_train, y_train, X_test, y_test, pi1=pi1, pi2=pi2)\n",
    "    \n",
    "    # RandomForestRegressor\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        criterion=\"poisson\",\n",
    "        min_samples_leaf=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    evaluate_rf(rf, X_train ,X_test, y_train, y_test, pi1=pi1, pi2=pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af26cbd-7efd-458a-bb17-6e9ceb4234b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253c4305-542b-48ca-900a-1e0471ce728b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "n=100\n",
    "pi1=0.2\n",
    "pi2=0.2\n",
    "print(f'n={n}_pi1={pi1}_pi2-{pi2}')\n",
    "\n",
    "fit_all(n, pi1, pi2)\n",
    "with open(f'n={n}_pi1={pi1}_pi2={pi2}.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(output.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc57a0b-99c4-4965-affa-4fe9f68f1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "nn = [50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "pi1 = pi2 = 0.4\n",
    "\n",
    "for n in nn:\n",
    "    buffer = io.StringIO()\n",
    "\n",
    "    with redirect_stdout(buffer):\n",
    "        print(f'n={n}_pi1={pi1}_pi2={pi2}')\n",
    "        fit_all(n, pi1, pi2)\n",
    "\n",
    "    # Get only this iteration's output\n",
    "    iteration_output = buffer.getvalue()\n",
    "\n",
    "    # Save to a file\n",
    "    with open(f'results_txt/n={n}_pi1={pi1}_pi2={pi2}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(iteration_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a9444-1500-4349-8fb0-d1d48426c50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
